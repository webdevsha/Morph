URL: https://course.aisafetyfundamentals.com/home/governance?session=4
---
![](<Base64-Image-Removed>)

In this course, we examine risks posed by increasingly advanced AI systems, ideas for addressing these risks through standards and regulation, and foreign policy approaches to promoting AI safety internationally. There is much uncertainty and debate about these issues, but the following resources offer a useful entry point for understanding how to reduce extreme risks in the current AI governance landscape.

Learn more about the facilitated version and apply [here](https://aisafetyfundamentals.com/governance)!

UNIT 4

## The AI Policy Toolkit

In this unit, we’ll discuss the goals governments might have and the levers they can use to achieve them.

A policy framework identifies high-level goals and lenses used to craft policy. Policy levers are tools and mechanisms that governments and organizations use to achieve specific goals. Governments have a variety of levers at their disposal to steer the development and deployment of AI systems. Policies could be scoped to control today’s AI systems, or they could try to control future ones. They could be highly targeted at specific risks or they could be broadly scoped.

Keep in mind that any framework is contentious. By setting the goals you should pursue, the authors are making value judgments and policy choices. Read these frameworks critically, considering how the author's perspective might influence their priorities.

##### By the end of the unit, you should be able to:

- Evaluate different frameworks and policy levers governments use to steer AI
- Explain the tradeoffs governments might make when balancing different priorities
- Identify the different stages of the AI lifecycle
- Describe the societal adaptation approach to AI governance

#### Resources (~1 hr 55 min)

[**The Policy Playbook: Building a Systems-Oriented Approach to Technology and National Security Policy**](https://cset.georgetown.edu/wp-content/uploads/The-Policy-Playbook.pdf) by Jack Corrigan, Melissa Flagg, and Dewi Murdick (2023)

Report

Estimated time: 45 mins

This report by the Center for Security and Emerging Technology first analyzes the tensions and tradeoffs between three strategic technology and national security goals: driving technological innovation, impeding adversaries’ progress, and promoting safe deployment. It then identifies different direct and enabling policy levers, assessing each based on the tradeoffs they make.

While this document is designed for US policymakers, most of its findings are broadly applicable.

[Listen to the narrated version](https://open.spotify.com/episode/72YiAHMT1ee6d4RSiCX31a?si=d35ccdb9ab404eac)

[**Strengthening Resilience to AI Risk: A Guide for UK Policymakers**](https://cetas.turing.ac.uk/sites/default/files/2023-08/cetas-cltr_ai_risk_briefing_paper.pdf) by Ardi Janjeva, Nikhil Mulani, Rosamund Powell, Jess Whittlestone, and Shahar Avin (2023)

Report

Estimated time: 30 mins

**Read page 15 (AI Risk Pathways) and all of Section 2**

This report from the Centre for Emerging Technology and Security and the Centre for Long-Term Resilience identifies different levers as they apply to different stages of the AI life cycle. They split the AI lifecycle into three stages: design, training, and testing; deployment and usage; and longer-term deployment and diffusion. It also introduces a risk mitigation hierarchy to rank different approaches in decreasing preference, arguing that “policy interventions will be most effective if they intervene at the point in the lifecycle where risk first arises.”

While this document is designed for UK policymakers, most of its findings are broadly applicable.

[Listen to the narrated version](https://open.spotify.com/episode/5HXFP8MeSCK0iv5RiWmvB1?si=KOwnPeSzSgOA5tp8QFW3cQ)

[**Resilience and Adaptation to Advanced AI**](https://substack.com/@jbernardi/p-147309945) by Jamie Bernardi (2024)

Blog

Estimated time: 10 mins

This post discusses the necessity for societies to adapt to the rapid advancements in AI. It emphasizes that while current strategies often focus on controlling AI development and diffusion, a complementary approach is essential: enhancing societal resilience to the negative impacts of AI. Bernardi introduces a framework to identify interventions that can prevent, defend against, and remedy harmful AI applications. For the full academic paper this post is summarizing, click [here](https://arxiv.org/abs/2405.10295).

[" target=\_blank>Listen to the narrated version](https://course.aisafetyfundamentals.com/home/Listen%20to%20the%20narrated%20version%20%3Cdiv%20class=)

[**Historical case studies of technology governance and international agreements**](https://www.aisafetyfundamentals.com/blog/historical-case-studies/) by BlueDot Impact (2023)

Article

Estimated time: 30 mins

**If you’re short on time, read just the first section on nuclear arms control.**

International AI governance may have lessons to learn from the histories of arms control, other general-purpose technologies, and attempts at forming environmental treaties. These compiled excerpts summarize these histories, emphasizing implications for AI. As you’re reading, consider what these cases reveal about the benefits and feasibility of international agreements as a policy lever for global AI governance.

[Listen to the narrated version](https://open.spotify.com/episode/2PshhFz79v30YUv3l3YWvu?si=G0GLjPgARyeu7Pm5o94hSg)

#### Exercises

Login or create a free account to complete the individual learnings!

#### Podcast Summary

#### Optional Resources

#### Unit Feedback

We are constantly updating the course and will use your feedback to update the individual learnings and activities for the upcoming units.

We'd appreciate if you completed this after your facilitated group discussion!

![A project by BlueDot Impact](<Base64-Image-Removed>)

We run courses that support you to develop the knowledge, community and network needed to pursue a high-impact career.

See a bug or have a feature request? Let us know!

© 2024 BlueDot Impact. All rights reserved.

COURSE

[Apply](https://forms.bluedot.org/BSUqN3WHmeL9MbzAj2P6) [Projects](https://aisafetyfundamentals.com/projects) [Facilitate](https://aisafetyfundamentals.com/facilitate)

BLUEDOT

[About us](https://bluedot.org/) [Contact](https://bluedot.org/contact/) [AI Safety](https://aisafetyfundamentals.com/) [Biosecurity](https://biosecurityfundamentals.com/) [Support us](https://donate.stripe.com/5kA3fpgjpdJv6o89AA)

By using this website, you agree to [our use](https://www.bluedotimpact.org/cookie-policy/) of required cookies.We also use analytics and performance cookies to provide you with a great experience and to help our website run effectively. To consent to [our use](https://www.bluedotimpact.org/cookie-policy/) of these cookies, click accept.

AcceptReject

Curriculum

Log in

![](https://v1.whalesyncusercontent.com/v1/c10143aacacd71fc6c92e9b7/a452b8a395c72b0abf2a6c14/4d6d00587d8d7077cad4c8a3/AISF-Logo.png)

#### AI Governance Course

UNIT 0

Icebreaker

UNIT 1

How AI Systems Work

Resources

0 out of 5 complete

Exercises

0 out of 3 complete

UNIT 2

The Promise and Perils of AI

Resources

0 out of 7 complete

Exercises

0 out of 4 complete